{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ ƒêang ch·∫°y tr√™n: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# ---- C·∫§U H√åNH ----\n",
    "INPUT_FILE = \"data.xlsx\"  # File d·ªØ li·ªáu ƒë·∫ßu v√†o (CSV ho·∫∑c Excel)\n",
    "OUTPUT_FILE = \"summary_results.json\"  # File l∆∞u k·∫øt qu·∫£\n",
    "MAX_WORDS_PER_CHUNK = 500  # Gi·ªõi h·∫°n t·ª´ m·ªói ƒëo·∫°n t√≥m t·∫Øt\n",
    "\n",
    "# ---- KI·ªÇM TRA GPU ----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üîπ ƒêang ch·∫°y tr√™n: {device}\")\n",
    "\n",
    "# ---- LOAD M√î H√åNH T√ìM T·∫ÆT ----\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "# ---- H√ÄM T√ìM T·∫ÆT ----\n",
    "def summarize_text(text, max_length=150, min_length=50):\n",
    "    \"\"\"T√≥m t·∫Øt m·ªôt ƒëo·∫°n vƒÉn b·∫£n\"\"\"\n",
    "    return summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "\n",
    "def extract_text(html_content):\n",
    "    \"\"\"L·∫•y vƒÉn b·∫£n s·∫°ch t·ª´ n·ªôi dung HTML\"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "def chunk_text(text, max_words=MAX_WORDS_PER_CHUNK):\n",
    "    \"\"\"Chia nh·ªè vƒÉn b·∫£n d√†i ƒë·ªÉ t√≥m t·∫Øt\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def summarize_long_text(text):\n",
    "    \"\"\"T√≥m t·∫Øt n·ªôi dung d√†i b·∫±ng c√°ch chia nh·ªè v√† t·ªïng h·ª£p\"\"\"\n",
    "    chunks = chunk_text(text)\n",
    "    summaries = [summarize_text(chunk) for chunk in chunks]\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "def process_document(row):\n",
    "    \"\"\"T√≥m t·∫Øt m·ªôt d√≤ng d·ªØ li·ªáu\"\"\"\n",
    "    raw_text = extract_text(row[\"Content\"])\n",
    "    summary = summarize_long_text(raw_text)\n",
    "\n",
    "    return {\n",
    "        \"title\": row[\"Title\"],\n",
    "        \"inner_title\": row[\"Inner Title\"],\n",
    "        \"link\": row[\"Link\"],\n",
    "        \"summary\": summary\n",
    "    }\n",
    "\n",
    "def process_all_data(num_workers=4):\n",
    "    \"\"\"X·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu v√† l∆∞u k·∫øt qu·∫£\"\"\"\n",
    "    data = pd.read_csv('courses_data/scikit_learn.csv')\n",
    "    \n",
    "    with Pool(num_workers) as p:  # Ch·∫°y ƒëa lu·ªìng\n",
    "        results = p.map(process_document, data)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('out.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
